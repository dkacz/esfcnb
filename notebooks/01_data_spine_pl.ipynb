{"cells": [{"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# --- Project root bootstrap for imports (works in VS Code, WSL, headless) ---\n", "import sys, importlib.util\n", "from pathlib import Path\n", "\n", "def find_repo_root() -> Path:\n", "    \"\"\"\n", "    Walk up from CWD to locate the project root by markers:\n", "    scripts/sfc_pl_runner.py and config/sfc_pl_runner.yml.\n", "    \"\"\"\n", "    here = Path.cwd().resolve()\n", "    for parent in (here, *here.parents):\n", "        if (parent / 'scripts' / 'sfc_pl_runner.py').exists() and (parent / 'config' / 'sfc_pl_runner.yml').exists():\n", "            return parent\n", "    raise FileNotFoundError('Could not find project root with scripts/sfc_pl_runner.py and config/sfc_pl_runner.yml')\n", "\n", "ROOT = find_repo_root()\n", "if str(ROOT) not in sys.path:\n", "    sys.path.insert(0, str(ROOT))\n", "print('Project root:', ROOT)\n", "\n", "# Primary import (package import requires scripts/__init__.py)\n", "try:\n", "    from scripts.sfc_pl_runner import load_config, run_from_config, verify\n", "    print('Imported from package:', ROOT / 'scripts' / 'sfc_pl_runner.py')\n", "except ModuleNotFoundError:\n", "    # Absolute path fallback (rarely needed if sys.path is set and scripts/__init__.py exists)\n", "    mfp = ROOT / 'scripts' / 'sfc_pl_runner.py'\n", "    spec = importlib.util.spec_from_file_location('scripts.sfc_pl_runner', str(mfp))\n", "    mod = importlib.util.module_from_spec(spec)\n", "    spec.loader.exec_module(mod)  # type: ignore[attr-defined]\n", "    load_config, run_from_config, verify = mod.load_config, mod.run_from_config, mod.verify\n", "    print('Imported via file path:', mfp)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Build B9F/B9FX9 subset for anchor and save parquet\nfrom pathlib import Path\nimport pandas as pd\nfrom src.sdmx_helpers import get_dsd, fetch_series\nROOT = ROOT\nproc = ROOT / 'data' / 'processed'\nproc.mkdir(parents=True, exist_ok=True)\nfp_ftr = proc / 'estat_nasq_10_f_tr_PL.parquet'\nfp_b9f = proc / 'estat_nasq_10_f_tr_b9f_PL.parquet'\ndf_b9f = pd.DataFrame()\nif fp_ftr.exists():\n    dff = pd.read_parquet(fp_ftr)\n    if 'na_item' in dff.columns:\n        df_b9f = dff[dff['na_item'].astype(str).isin(['B9F','B9FX9'])].copy()\nif df_b9f is None or df_b9f.empty:\n    try:\n        info = get_dsd('ESTAT','NASQ_10_F_TR')\n        units = [u for u in ['MIO_EUR','CP_MEUR'] if u in info.codes.get('unit',[])] or ['MIO_EUR']\n        finpos_opts = [c for c in ['NET','ASS','LIAB'] if c in info.codes.get('finpos',[])]\n        frames=[]\n        for unit in units:\n            tried=[None]\n            if 'NET' in finpos_opts: tried=[\"NET\"]\n            for fin in tried:\n                filt={'freq':'Q','unit':unit,'sector':['S13'],'na_item':['B9F','B9FX9'],'geo':'PL'}\n                if fin: filt['finpos']=[fin]\n                try:\n                    dfp = fetch_series('NASQ_10_F_TR', filt, agency='ESTAT')\n                    if dfp is not None and not dfp.empty:\n                        frames.append(dfp)\n                except Exception:\n                    pass\n        if frames:\n            import pandas as pd\n            df_b9f = pd.concat(frames, ignore_index=True)\n    except Exception:\n        pass\nif df_b9f is not None and not df_b9f.empty:\n    df_b9f.to_parquet(fp_b9f)\nprint('B9F rows:', 0 if df_b9f is None else len(df_b9f))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Derive NF_TR check_items and windows; save nf_tr_meta.json\nfrom pathlib import Path\nimport json, pandas as pd\nROOT=ROOT\nproc = ROOT / 'data' / 'processed'\nmeta_fp = proc / 'nf_tr_meta.json'\ndf_fp = proc / 'estat_nasq_10_nf_tr_PL.parquet'\nmeta={}\ntry:\n    if df_fp.exists():\n        df = pd.read_parquet(df_fp)\n        core=['S11','S12','S13','S14_S15']\n        if set(['time','na_item','direct','sector']).issubset(df.columns):\n            d = df[df['sector'].isin(core)].copy()\n            piv = d.pivot_table(index=['time','na_item'], columns='direct', values='value', aggfunc='sum').fillna(0.0)\n            cols=[c for c in ['RECV','PAID'] if c in piv.columns]\n            items=[]; win={}\n            if len(cols)==2:\n                mask=(piv[cols[0]]!=0)&(piv[cols[1]]!=0)\n                idx=piv[mask].reset_index()[['time','na_item']]\n                items=sorted(idx['na_item'].astype(str).unique().tolist())\n                for itm, sub in idx.groupby('na_item'):\n                    win[str(itm)]={'first_quarter': str(sub['time'].min()), 'last_quarter': str(sub['time'].max())}\n            meta={'sectors_direct': core,'na_items_final': items,'check_items': items,'check_item_windows': win}\n            meta_fp.write_text(json.dumps(meta, indent=2), encoding='utf-8')\nexcept Exception:\n    pass\nprint('NF_TR check_items:', [] if not meta else meta.get('check_items'))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# STEP 1.4 \u2014 ECB QSA Loans F4 (PL): whom-to-whom minimal panel\nfrom pathlib import Path\nimport pandas as pd\nfrom src.sdmx_helpers import get_dsd, fetch_series\nROOT = ROOT\nproc = ROOT / 'data' / 'processed'\nproc.mkdir(parents=True, exist_ok=True)\ntry:\n    info = get_dsd('ECB','QSA')\n    dims = info.dimensions\n    codes = info.codes\n    def has(dim, vals):\n        return dim in codes and all(v in set(codes[dim]) for v in vals)\n    instr_dim = next((d for d in dims if has(d,['F4'])), None)\n    entry_dim = next((d for d in dims if has(d,['A','L']) or has(d,['ASS','LIAB'])), None)\n    stock_dim = next((d for d in dims if has(d,['LE']) or has(d,['S','F'])), None)\n    unit_dim = next((d for d in dims if has(d,['MIO_EUR']) or has(d,['CP_MEUR'])), None)\n    geo_dim = next((d for d in dims if has(d,['PL'])), None)\n    cons_dim = next((d for d in dims if has(d,['N']) or has(d,['NC'])), None)\n    sec_dims = [d for d in dims if has(d,['S11','S12','S13','S14_S15'])][:2]\n    flt={'freq':'Q'}\n    if unit_dim: flt[unit_dim]='MIO_EUR'\n    if geo_dim: flt[geo_dim]='PL'\n    if instr_dim: flt[instr_dim]='F4'\n    if entry_dim: flt[entry_dim]=['A','L']\n    if stock_dim:\n        if has(stock_dim,['LE']): flt[stock_dim]='LE'\n        else: flt[stock_dim]=['S','F']\n    if cons_dim: flt[cons_dim]='N'\n    if len(sec_dims)==2:\n        flt[sec_dims[0]]=['S11','S12','S13','S14_S15']\n        flt[sec_dims[1]]=['S11','S12','S13','S14_S15']\n    df_qsa = fetch_series('QSA', flt, agency='ECB')\n    if df_qsa is None or df_qsa.empty:\n        df_qsa = pd.DataFrame()\n    else:\n        # normalize column names\n        ren={}\n        if instr_dim: ren[instr_dim]='instrument'\n        if entry_dim: ren[entry_dim]='entry'\n        if stock_dim: ren[stock_dim]='stock_flow_flag'\n        if len(sec_dims)==2:\n            ren[sec_dims[0]]='ref_sector'; ren[sec_dims[1]]='cp_sector'\n        df_qsa = df_qsa.rename(columns=ren)\n        keep=['time','ref_sector','cp_sector','instrument','entry','stock_flow_flag','unit','value','dataset','last_update']\n        for c in keep:\n            if c not in df_qsa.columns: df_qsa[c]=None\n        df_qsa = df_qsa[keep]\n    if not df_qsa.empty:\n        df_qsa.to_parquet(proc / 'ecb_QSA_PL.parquet')\n    print('QSA rows:', 0 if df_qsa is None else len(df_qsa))\nexcept Exception as e:\n    print('QSA pull failed:', e)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# STEP 1.4 \u2014 Eurostat revaluations (K.7) and other changes (K.1\u2013K.6)\nfrom pathlib import Path\nimport pandas as pd\nfrom src.sdmx_helpers import get_dsd, fetch_series\nROOT = ROOT\nproc = ROOT / 'data' / 'processed'\nproc.mkdir(parents=True, exist_ok=True)\n\ndef _pull_estat(dataset):\n    try:\n        info = get_dsd('ESTAT', dataset)\n    except Exception:\n        return pd.DataFrame()\n    unit = next((u for u in ['MIO_EUR','CP_MEUR'] if u in info.codes.get('unit',[])), None)\n    if not unit:\n        unit='MIO_EUR'\n    if dataset.endswith('_F_GL'):\n        items=[c for c in info.codes.get('na_item',[]) if str(c).startswith('K7') or str(c) in ('K7','K.7')]\n    else:\n        items=[c for c in info.codes.get('na_item',[]) if str(c).startswith('K')]\n    if not items: items = ['K7'] if dataset.endswith('_F_GL') else ['K1','K2','K3','K4','K5','K6']\n    filters={'freq':'Q','unit':unit,'sector':['S1','S11','S12','S13','S14_S15','S2'],'na_item':items,'geo':'PL'}\n    try:\n        return fetch_series(dataset, filters, agency='ESTAT')\n    except Exception:\n        return pd.DataFrame()\n\ndf_gl=_pull_estat('NASQ_10_F_GL')\ndf_oc=_pull_estat('NASQ_10_F_OC')\nif df_gl is not None and not df_gl.empty:\n    df_gl.to_parquet(proc / 'estat_nasq_10_f_gl_PL.parquet')\nif df_oc is not None and not df_oc.empty:\n    df_oc.to_parquet(proc / 'estat_nasq_10_f_oc_PL.parquet')\nprint('F_GL rows:', 0 if df_gl is None else len(df_gl), '| F_OC rows:', 0 if df_oc is None else len(df_oc))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["CFG = load_config()\n", "rc = verify(CFG)\n", "print('verify() exit code:', rc)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Data Spine \u2014 Poland (Eurostat + ECB)\n", "\n", "This notebook orchestrates Step 1 pulls and QC via the fixed runner."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### VERIFY & FIX (STEP 1.2)\n", "\n", "Run All to pull SDMX data and print the verification block."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# (Optional) full pipeline from notebook (skipped by default)\n", "import os\n", "if os.environ.get('NOTEBOOK_PULL')=='1':\n", "    _ = run_from_config(CFG)\n", "else:\n", "    print('Skipping full pull (set NOTEBOOK_PULL=1 to enable)')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Single-source verification print (JSON-driven)\n# Always call verify() at least once so it writes the snapshot JSON and exit code.\nrc = verify(CFG)\nprint(f\"verify() exit code: {rc}\")\n\n# The rest of the display must be built from the freshly written JSON.\nimport json, pandas as pd\nfrom pathlib import Path\nvr = json.loads((ROOT / 'data' / 'processed' / 'verification_report.json').read_text(encoding='utf-8'))\n\n# --- Artifacts table ---\nart_rows = []\nfor p, meta in vr.get('artifacts', {}).items():\n    art_rows.append({\n        'path': p,\n        'exists': meta.get('exists'),\n        'size_bytes': meta.get('size_bytes'),\n        'rows': meta.get('rows'),\n        'cols': meta.get('cols'),\n        'first_quarter': meta.get('first_quarter'),\n        'last_quarter': meta.get('last_quarter'),\n    })\nart_df = pd.DataFrame(art_rows).sort_values('path')\ndisplay(art_df)\n\n# Echo NF_TR scope, QC, anchors, failures from JSON\ndisplay(vr.get('nf_tr_scope', {}))\ndisplay(vr.get('qc_summary', {}))\ndisplay(vr.get('anchors', {}))\ndisplay(vr.get('failures', {}))\n\n# Checksums (file & sample) from JSON\ndisplay(vr.get('checksums', {}))\n\n# Hard stop on failure (notebook)\nVERIFY_RC = rc\n# soft-fail: do not abort; proof cell will still run\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Re-run verification only (idempotent); prints full block\n", "from scripts.sfc_pl_runner import verify\n", "rc = verify(CFG)\nprint('verify() returned:', rc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Output Gate & Debug Pack \u2014 Self-contained\n", "Run to print the artifacts table, schema checks, NF_TR scope, QC summary, anchors, and failure lines."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print the full verification block (includes artifacts, schema, scope, QC, anchors, failures)\n", "from scripts.sfc_pl_runner import verify\n", "rc = verify(CFG)\nprint('verify() returned:', rc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convenience: print single worst stocks-bridge line + one failure log line if any\n", "import json, re, pathlib\n", "qc=json.load(open('data/processed/qc_summary.json')) if pathlib.Path('data/processed/qc_summary.json').exists() else {}\n", "worst=(qc.get('stocks_bridge',{}).get('worst10') or [])\n", "print('WORST stocks-bridge line:', worst[0] if worst else None)\n", "# Show first fetch failure logged\n", "log_lines=[]\n", "try:\n", "    with open('logs/sfc_pl_runner.log','r',encoding='utf-8') as fh:\n", "        for ln in fh:\n", "            if ln.strip().startswith('FAIL | dataset='):\n", "                log_lines.append(ln.strip())\n", "except FileNotFoundError:\n", "    pass\n", "print('ONE failure line (if any):', log_lines[0] if log_lines else None)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# --- Notebook run proof: emit a runtime token and bind it to the JSON snapshot ---\nimport os, sys, json, time, hashlib, platform\nfrom pathlib import Path\n\nvr_path = (ROOT / 'data' / 'processed' / 'verification_report.json')\nassert vr_path.exists(), \"verification_report.json not found; run verify(CFG) first\"\nvr_bytes = vr_path.read_bytes()\nvr_sha = hashlib.sha256(vr_bytes).hexdigest()\n\n# Runtime-only token (cannot exist without execution)\nnonce = os.urandom(32)\nnow_ns = time.time_ns()\nproof_token = hashlib.sha256(nonce + str(now_ns).encode('utf-8')).hexdigest()[:16]\n\nproof = {\n    \"run_utc\": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n    \"cwd\": str(Path.cwd().resolve()),\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"verification_report_sha256\": vr_sha,\n    \"proof_token\": proof_token,\n}\n\nproof_path = ROOT / 'data' / 'processed' / '_nb_run_proof.json'\nproof_path.parent.mkdir(parents=True, exist_ok=True)\nproof_path.write_text(json.dumps(proof, indent=2), encoding='utf-8')\nprint('NOTEBOOK_RUN_PROOF', proof_token, vr_sha)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}, "jupytext": {"formats": "ipynb,py:percent", "text_representation": {"extension": ".py", "format_name": "percent"}}}, "nbformat": 4, "nbformat_minor": 4}