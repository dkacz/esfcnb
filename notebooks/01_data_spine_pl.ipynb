{"cells": [{"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# --- Project root bootstrap for imports (works in VS Code, WSL, headless) ---\n", "import sys, importlib.util\n", "from pathlib import Path\n", "\n", "def find_repo_root() -> Path:\n", "    \"\"\"\n", "    Walk up from CWD to locate the project root by markers:\n", "    scripts/sfc_pl_runner.py and config/sfc_pl_runner.yml.\n", "    \"\"\"\n", "    here = Path.cwd().resolve()\n", "    for parent in (here, *here.parents):\n", "        if (parent / 'scripts' / 'sfc_pl_runner.py').exists() and (parent / 'config' / 'sfc_pl_runner.yml').exists():\n", "            return parent\n", "    raise FileNotFoundError('Could not find project root with scripts/sfc_pl_runner.py and config/sfc_pl_runner.yml')\n", "\n", "ROOT = find_repo_root()\n", "if str(ROOT) not in sys.path:\n", "    sys.path.insert(0, str(ROOT))\n", "print('Project root:', ROOT)\n", "\n", "# Primary import (package import requires scripts/__init__.py)\n", "try:\n", "    from scripts.sfc_pl_runner import load_config, run_from_config, verify\n", "    print('Imported from package:', ROOT / 'scripts' / 'sfc_pl_runner.py')\n", "except ModuleNotFoundError:\n", "    # Absolute path fallback (rarely needed if sys.path is set and scripts/__init__.py exists)\n", "    mfp = ROOT / 'scripts' / 'sfc_pl_runner.py'\n", "    spec = importlib.util.spec_from_file_location('scripts.sfc_pl_runner', str(mfp))\n", "    mod = importlib.util.module_from_spec(spec)\n", "    spec.loader.exec_module(mod)  # type: ignore[attr-defined]\n", "    load_config, run_from_config, verify = mod.load_config, mod.run_from_config, mod.verify\n", "    print('Imported via file path:', mfp)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Build B9F/B9FX9 subset for anchor and save parquet\nfrom pathlib import Path\nimport pandas as pd\nfrom src.sdmx_helpers import get_dsd, fetch_series\nROOT = ROOT\nproc = ROOT / 'data' / 'processed'\nproc.mkdir(parents=True, exist_ok=True)\nfp_ftr = proc / 'estat_nasq_10_f_tr_PL.parquet'\nfp_b9f = proc / 'estat_nasq_10_f_tr_b9f_PL.parquet'\ndf_b9f = pd.DataFrame()\nif fp_ftr.exists():\n    dff = pd.read_parquet(fp_ftr)\n    if 'na_item' in dff.columns:\n        df_b9f = dff[dff['na_item'].astype(str).isin(['B9F','B9FX9'])].copy()\nif df_b9f is None or df_b9f.empty:\n    try:\n        info = get_dsd('ESTAT','NASQ_10_F_TR')\n        units = [u for u in ['MIO_EUR','CP_MEUR'] if u in info.codes.get('unit',[])] or ['MIO_EUR']\n        finpos_opts = [c for c in ['NET','ASS','LIAB'] if c in info.codes.get('finpos',[])]\n        frames=[]\n        for unit in units:\n            tried=[None]\n            if 'NET' in finpos_opts: tried=[\"NET\"]\n            for fin in tried:\n                filt={'freq':'Q','unit':unit,'sector':['S13'],'na_item':['B9F','B9FX9'],'geo':'PL'}\n                if fin: filt['finpos']=[fin]\n                try:\n                    dfp = fetch_series('NASQ_10_F_TR', filt, agency='ESTAT')\n                    if dfp is not None and not dfp.empty:\n                        frames.append(dfp)\n                except Exception:\n                    pass\n        if frames:\n            import pandas as pd\n            df_b9f = pd.concat(frames, ignore_index=True)\n    except Exception:\n        pass\nif df_b9f is not None and not df_b9f.empty:\n    df_b9f.to_parquet(fp_b9f)\nprint('B9F rows:', 0 if df_b9f is None else len(df_b9f))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Derive NF_TR check_items and windows; save nf_tr_meta.json\nfrom pathlib import Path\nimport json, pandas as pd\nROOT=ROOT\nproc = ROOT / 'data' / 'processed'\nmeta_fp = proc / 'nf_tr_meta.json'\ndf_fp = proc / 'estat_nasq_10_nf_tr_PL.parquet'\nmeta={}\ntry:\n    if df_fp.exists():\n        df = pd.read_parquet(df_fp)\n        core=['S11','S12','S13','S14_S15']\n        if set(['time','na_item','direct','sector']).issubset(df.columns):\n            d = df[df['sector'].isin(core)].copy()\n            piv = d.pivot_table(index=['time','na_item'], columns='direct', values='value', aggfunc='sum').fillna(0.0)\n            cols=[c for c in ['RECV','PAID'] if c in piv.columns]\n            items=[]; win={}\n            if len(cols)==2:\n                mask=(piv[cols[0]]!=0)&(piv[cols[1]]!=0)\n                idx=piv[mask].reset_index()[['time','na_item']]\n                items=sorted(idx['na_item'].astype(str).unique().tolist())\n                for itm, sub in idx.groupby('na_item'):\n                    win[str(itm)]={'first_quarter': str(sub['time'].min()), 'last_quarter': str(sub['time'].max())}\n            meta={'sectors_direct': core,'na_items_final': items,'check_items': items,'check_item_windows': win}\n            meta_fp.write_text(json.dumps(meta, indent=2), encoding='utf-8')\nexcept Exception:\n    pass\nprint('NF_TR check_items:', [] if not meta else meta.get('check_items'))\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["CFG = load_config()\n", "rc = verify(CFG)\n", "print('verify() exit code:', rc)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Data Spine \u2014 Poland (Eurostat + ECB)\n", "\n", "This notebook orchestrates Step 1 pulls and QC via the fixed runner."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### VERIFY & FIX (STEP 1.2)\n", "\n", "Run All to pull SDMX data and print the verification block."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# (Optional) full pipeline from notebook (skipped by default)\n", "import os\n", "if os.environ.get('NOTEBOOK_PULL')=='1':\n", "    _ = run_from_config(CFG)\n", "else:\n", "    print('Skipping full pull (set NOTEBOOK_PULL=1 to enable)')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Single-source verification print (JSON-driven)\n# Always call verify() at least once so it writes the snapshot JSON and exit code.\nrc = verify(CFG)\nprint(f\"verify() exit code: {rc}\")\n\n# The rest of the display must be built from the freshly written JSON.\nimport json, pandas as pd\nfrom pathlib import Path\nvr = json.loads((ROOT / 'data' / 'processed' / 'verification_report.json').read_text(encoding='utf-8'))\n\n# --- Artifacts table ---\nart_rows = []\nfor p, meta in vr.get('artifacts', {}).items():\n    art_rows.append({\n        'path': p,\n        'exists': meta.get('exists'),\n        'size_bytes': meta.get('size_bytes'),\n        'rows': meta.get('rows'),\n        'cols': meta.get('cols'),\n        'first_quarter': meta.get('first_quarter'),\n        'last_quarter': meta.get('last_quarter'),\n    })\nart_df = pd.DataFrame(art_rows).sort_values('path')\ndisplay(art_df)\n\n# Echo NF_TR scope, QC, anchors, failures from JSON\ndisplay(vr.get('nf_tr_scope', {}))\ndisplay(vr.get('qc_summary', {}))\ndisplay(vr.get('anchors', {}))\ndisplay(vr.get('failures', {}))\n\n# Checksums (file & sample) from JSON\ndisplay(vr.get('checksums', {}))\n\n# Hard stop on failure (notebook)\nVERIFY_RC = rc\n# soft-fail: do not abort; proof cell will still run\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Re-run verification only (idempotent); prints full block\n", "from scripts.sfc_pl_runner import verify\n", "rc = verify(CFG)\nprint('verify() returned:', rc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Output Gate & Debug Pack \u2014 Self-contained\n", "Run to print the artifacts table, schema checks, NF_TR scope, QC summary, anchors, and failure lines."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print the full verification block (includes artifacts, schema, scope, QC, anchors, failures)\n", "from scripts.sfc_pl_runner import verify\n", "rc = verify(CFG)\nprint('verify() returned:', rc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convenience: print single worst stocks-bridge line + one failure log line if any\n", "import json, re, pathlib\n", "qc=json.load(open('data/processed/qc_summary.json')) if pathlib.Path('data/processed/qc_summary.json').exists() else {}\n", "worst=(qc.get('stocks_bridge',{}).get('worst10') or [])\n", "print('WORST stocks-bridge line:', worst[0] if worst else None)\n", "# Show first fetch failure logged\n", "log_lines=[]\n", "try:\n", "    with open('logs/sfc_pl_runner.log','r',encoding='utf-8') as fh:\n", "        for ln in fh:\n", "            if ln.strip().startswith('FAIL | dataset='):\n", "                log_lines.append(ln.strip())\n", "except FileNotFoundError:\n", "    pass\n", "print('ONE failure line (if any):', log_lines[0] if log_lines else None)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# --- Notebook run proof: emit a runtime token and bind it to the JSON snapshot ---\nimport os, sys, json, time, hashlib, platform\nfrom pathlib import Path\n\nvr_path = (ROOT / 'data' / 'processed' / 'verification_report.json')\nassert vr_path.exists(), \"verification_report.json not found; run verify(CFG) first\"\nvr_bytes = vr_path.read_bytes()\nvr_sha = hashlib.sha256(vr_bytes).hexdigest()\n\n# Runtime-only token (cannot exist without execution)\nnonce = os.urandom(32)\nnow_ns = time.time_ns()\nproof_token = hashlib.sha256(nonce + str(now_ns).encode('utf-8')).hexdigest()[:16]\n\nproof = {\n    \"run_utc\": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n    \"cwd\": str(Path.cwd().resolve()),\n    \"python\": sys.version,\n    \"platform\": platform.platform(),\n    \"verification_report_sha256\": vr_sha,\n    \"proof_token\": proof_token,\n}\n\nproof_path = ROOT / 'data' / 'processed' / '_nb_run_proof.json'\nproof_path.parent.mkdir(parents=True, exist_ok=True)\nproof_path.write_text(json.dumps(proof, indent=2), encoding='utf-8')\nprint('NOTEBOOK_RUN_PROOF', proof_token, vr_sha)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}, "jupytext": {"formats": "ipynb,py:percent", "text_representation": {"extension": ".py", "format_name": "percent"}}}, "nbformat": 4, "nbformat_minor": 4}